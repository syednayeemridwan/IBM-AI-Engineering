{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prevents neural networks from booming sooner\n",
    "- Happens when sigmoid activation function is used\n",
    "- Sigmoid function makes the intermediate values of the neural network between 0 and 1 \n",
    "- When back-propagation is used, we keep multiplying values less than 1 with each other. \n",
    "- So the gradient keeps getting smaller and smaller moving backward to the network.\n",
    "- So, the neurons in the earlier layers learn slowly compared to the neurons in the later layers in the network\n",
    "- Thus, training takes too long and accuracy is compromised."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding Gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Opposite to vanishing gradients\n",
    "- When intermediate values become very large\n",
    "- When back-propagation is used, we keep multiplying larger values with each other. Thus, the multiplication of these gradients will become huge over time. \n",
    "- This results in the model being unable to learn and its behavior becomes unstable. This problem is called the exploding gradient problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produces vanishing gradient when working with larger values of z that is close to positive or negative infinity\n",
    "- Values only range from 0 to 1 (not symmetric)\n",
    "- Values are always positive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaled version of sigmoid function\n",
    "- Values only range from -1 to 1 (symmetric)\n",
    "- Still vanishing gradient problem for deep neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most widely used \n",
    "- Nonlinear\n",
    "- Only used in hidden layers\n",
    "- Does not activate all neurons at the same time. (Only positive valued neurons are activated, while negative valued neurons are de-activated, making sparse neural activation of selected neurons one at a time)\n",
    "- Solves vanishing gradients\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transforms numeric values into probabilities\n",
    "- Used in multi-class classification"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
