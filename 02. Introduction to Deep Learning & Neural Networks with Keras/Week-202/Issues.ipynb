{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prevents neural networks from booming sooner\n",
    "- Happens when sigmoid activation function is used\n",
    "- Sigmoid function makes the intermediate values of the neural network between 0 and 1 \n",
    "- When back-propagation is used, we keep multiplying values less than 1 with each other. \n",
    "- So the gradient keeps getting smaller and smaller moving backward to the network.\n",
    "- So, the neurons in the earlier layers learn slowly compared to the neurons in the later layers in the network\n",
    "- Thus, training takes too long and accuracy is compromised."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding Gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Opposite to vanishing gradients\n",
    "- When intermediate values become very large\n",
    "- When back-propagation is used, we keep multiplying larger values with each other. Thus, the multiplication of these gradients will become huge over time. \n",
    "- This results in the model being unable to learn and its behavior becomes unstable. This problem is called the exploding gradient problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
