{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost / Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost function is a way to calculate the total error of the model.\n",
    "- It calculates the difference between actual and predicted values for all data in the training dataset. \n",
    "- The lower the cost, the better the model is at estimating labels. \n",
    "- Cost function tells us which model is the best fit\n",
    "- Optimization algorithms try to minimize the cost and find the optimized cost for the model at certain point like when should we stop estimating cost, how long should we iterate the process of estimating cost (eg: gradient descent). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function estimates the error of a model.\n",
    "Gradient descent is a technique that uses derivative of the cost function\n",
    "to change the parameter values (weights / co-efficients) to minimize the \n",
    "cost or error.\n",
    "\n",
    "Change the current weights by delta and take a step. The direction and size \n",
    "of step is calculated by the gradient (slope) of cost function at the \n",
    "current position by some specified learning rate. The gradient vector \n",
    "contains the slopes for all weight vectors / co-efficients. This gradient \n",
    "vector is used to update all existing weights.\n",
    "\n",
    "If slope is negative, we are stepping downhill and will reach a \n",
    "minimum position when the slope becomes 0 / algorithm converges.\n",
    "If slope is positive, we are stepping uphill and will reach a \n",
    "maximum position when the slope becomes 0 / algorithm converges.\n",
    "\n",
    "Process of applying gradient descent:\n",
    "1. Initialize parameters randomly\n",
    "2. Calculate cost for training set with a cost function\n",
    "3. Calculate gradient of the cost function (partial derivative for all dataset)\n",
    "4. Update weights with new values\n",
    "5. Repeat from step 2 until cost is small enough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we do forward propagation, the output always have some sort of error (the computed output is way off the real output)\n",
    "- The difference between the real and computed output is error\n",
    "- Back propagation is a method of updating weights and bias of inputs through introducing this error from the output to the whole network so that the error is reduced.\n",
    "- This is actually another way of applying gradient descent to each individual neurons so that the weights and bias are updated for optimized result. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- E = Average error\n",
    "- a = Activation function\n",
    "- z = output\n",
    "- derivatives = update of these through back-propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.05.png\"  style=\"width: 400px, height: 300px;\"/></center"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given,\n",
    "- Learning rate = 0.4\n",
    "- Real value = 0.25\n",
    "- threshold = 0.001\n",
    "- iterations = 1000\n",
    "- and other previous values from forward propagation steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01.06.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.07.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.08.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.09.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.10.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize weights and bias in the neural network\n",
    "2. Calculate network output using forward propagation\n",
    "3. Calculate error from real value and computed value\n",
    "4. Update weights and bias using back-propagation\n",
    "5. Repeat steps 2 to 4 until the error is below a certain threshold or a given number of iteration is done  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
