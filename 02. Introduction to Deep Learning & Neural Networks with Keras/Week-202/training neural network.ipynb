{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost / Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost function is a way to calculate the total error of the model.\n",
    "- It calculates the difference between actual and predicted values for all data in the training dataset. \n",
    "- The lower the cost, the better the model is at estimating labels. \n",
    "- Cost function tells us which model is the best fit\n",
    "- Optimization algorithms try to minimize the cost and find the optimized cost for the model at certain point like when should we stop estimating cost, how long should we iterate the process of estimating cost (eg: gradient descent). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function estimates the error of a model.\n",
    "Gradient descent is a technique that uses derivative of the cost function\n",
    "to change the parameter values (weights / co-efficients) to minimize the \n",
    "cost or error.\n",
    "\n",
    "Change the current weights by delta and take a step. The direction and size \n",
    "of step is calculated by the gradient (slope) of cost function at the \n",
    "current position by some specified learning rate. The gradient vector \n",
    "contains the slopes for all weight vectors / co-efficients. This gradient \n",
    "vector is used to update all existing weights.\n",
    "\n",
    "If slope is negative, we are stepping downhill and will reach a \n",
    "minimum position when the slope becomes 0 / algorithm converges.\n",
    "If slope is positive, we are stepping uphill and will reach a \n",
    "maximum position when the slope becomes 0 / algorithm converges.\n",
    "\n",
    "Process of applying gradient descent:\n",
    "1. Initialize parameters randomly\n",
    "2. Calculate cost for training set with a cost function\n",
    "3. Calculate gradient of the cost function (partial derivative for all dataset)\n",
    "4. Update weights with new values\n",
    "5. Repeat from step 2 until cost is small enough"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
